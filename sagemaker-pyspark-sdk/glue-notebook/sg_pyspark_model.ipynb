{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker_pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "# --- Step 0: Clear any empty/conflicting AWS environment variables ---\n",
    "# This ensures that the S3A connector does not pick up empty credentials.\n",
    "os.environ.pop(\"AWS_ACCESS_KEY_ID\", None)\n",
    "os.environ.pop(\"AWS_SECRET_ACCESS_KEY\", None)\n",
    "# ------------------------------------------\n",
    "# 1. Initialize SparkSession with SageMaker Jars\n",
    "# ------------------------------------------\n",
    "# This will add the SageMaker-related jar files to the Spark driver classpath.\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"XGBoostSageMakerExample\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", classpath) \\\n",
    "    .config(\"spark.driver.userClassPathFirst\", \"true\") \\\n",
    "    .config(\"spark.executor.userClassPathFirst\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"AKIAR3HUOTHCI6ZEO5FV\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"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\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "            \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data schema: StructType([StructField('label', DoubleType(), True), StructField('features', VectorUDT(), True)])\n"
     ]
    }
   ],
   "source": [
    "# 3. (Optional) Also explicitly set these properties in the Hadoop configuration.\n",
    "hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", \"AKIAR3HUOTHCI6ZEO5FV\")\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\",\n",
    "                \"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\")\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "hadoop_conf.set(\"fs.s3a.aws.credentials.provider\",\n",
    "                \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "# 4. Define region and S3 path for the training data.\n",
    "region = \"us-east-1\"\n",
    "data_path = \"s3a://sagemaker-sample-data-{}/spark/mnist/train/\".format(region)\n",
    "\n",
    "# 5. Load the training and test data in libsvm format.\n",
    "training_data = spark.read.format(\"libsvm\") \\\n",
    "    .option(\"numFeatures\", \"784\") \\\n",
    "    .load(data_path)\n",
    "\n",
    "test_data = spark.read.format(\"libsvm\") \\\n",
    "    .option(\"numFeatures\", \"784\") \\\n",
    "    .load(data_path)\n",
    "\n",
    "print(\"Training data schema:\", training_data.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 4. Configure the XGBoost SageMaker Estimator\n",
    "# ------------------------------------------\n",
    "from sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator\n",
    "from sagemaker_pyspark import IAMRole\n",
    "\n",
    "# Define the IAM role to use.\n",
    "iam_role = \"arn:aws:iam::127214197188:role/sagemaker-service-role\"  # <-- Update this!\n",
    "\n",
    "# Create an estimator instance with the desired configuration.\n",
    "# Note that the instance types and counts can be adjusted to suit your needs.\n",
    "xgboost_estimator = XGBoostSageMakerEstimator(\n",
    "    # trainingInstanceType=\"ml.m4.xlarge\",\n",
    "    trainingInstanceType=\"local\",\n",
    "    trainingInstanceCount=1,\n",
    "    # endpointInstanceType=\"ml.m4.xlarge\",\n",
    "    endpointInstanceType=\"local\",\n",
    "    endpointInitialInstanceCount=1,\n",
    "    sagemakerRole=IAMRole(iam_role)\n",
    ")\n",
    "\n",
    "# Set the hyperparameters required by the XGBoost algorithm.\n",
    "# In this example, we perform multi-class classification:\n",
    "# - 'multi:softmax' sets the objective for multi-class classification.\n",
    "# - 'numRound' is the number of boosting rounds.\n",
    "# - 'numClasses' defines the number of classes.\n",
    "xgboost_estimator.setObjective(\"multi:softmax\")\n",
    "xgboost_estimator.setNumRound(25)\n",
    "xgboost_estimator.setNumClasses(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------------------\n",
    "# # 5. Train the Model and Deploy as a SageMaker Endpoint\n",
    "# # ------------------------------------------\n",
    "# # Calling fit() will:\n",
    "# #   - Launch a SageMaker training job using the training_data.\n",
    "# #   - Deploy the resulting model as a hosted endpoint.\n",
    "# xgboost_model = xgboost_estimator.fit(training_data)\n",
    "\n",
    "# # ------------------------------------------\n",
    "# # 6. Use the Deployed Model for Predictions\n",
    "# # ------------------------------------------\n",
    "# # The returned xgboost_model is a SageMakerModel. Calling transform() sends the test data\n",
    "# # to the deployed endpoint and returns a DataFrame with predictions.\n",
    "# predictions = xgboost_model.transform(test_data)\n",
    "# predictions.show(truncate=False)\n",
    "\n",
    "# # ------------------------------------------\n",
    "# # Optional: Cleanup\n",
    "# # ------------------------------------------\n",
    "# # Depending on the SDK behavior, the endpoint might persist after this code runs.\n",
    "# # If you are done with the endpoint, make sure to delete it (either manually or via code)\n",
    "# # to avoid incurring ongoing costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/05 20:20:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x109e21120>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/projects/sagemaker-spark/venv/lib/python3.12/site-packages/pyspark/ml/wrapper.py\", line 53, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "                                              ^^^^^^^^^^^^^^\n",
      "AttributeError: 'XGBoostSageMakerEstimator' object has no attribute '_java_obj'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data schema:\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBoostSageMakerEstimator.__init__() got an unexpected keyword argument 'trainingImage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 71\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Configure the XGBoost SageMaker Estimator in local mode.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# For example, use the Amazon ECR image for XGBoost:1.0-1-cpu-py3.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# (Make sure you have Docker running so that local mode can pull and run this image.)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m training_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m382416733822.dkr.ecr.us-east-1.amazonaws.com/xgboost:1.0-1-cpu-py3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 71\u001b[0m xgboost_estimator \u001b[38;5;241m=\u001b[39m \u001b[43mXGBoostSageMakerEstimator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Run training in local (Docker) mode.\u001b[39;49;00m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainingInstanceType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainingInstanceCount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpointInstanceType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Run inference in local mode.\u001b[39;49;00m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpointInitialInstanceCount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43msagemakerRole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIAMRole\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marn:aws:iam::127214197188:role/sagemaker-service-role\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainingImage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_image\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Force use of the specified image.\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Set hyperparameters for multi-class classification.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m xgboost_estimator\u001b[38;5;241m.\u001b[39msetObjective(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti:softmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: XGBoostSageMakerEstimator.__init__() got an unexpected keyword argument 'trainingImage'"
     ]
    }
   ],
   "source": [
    "from sagemaker_pyspark import IAMRole\n",
    "from sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator\n",
    "from pyspark.sql import SparkSession\n",
    "import sagemaker_pyspark\n",
    "import os\n",
    "# Clear any conflicting AWS environment variables.\n",
    "os.environ.pop(\"AWS_ACCESS_KEY_ID\", None)\n",
    "os.environ.pop(\"AWS_SECRET_ACCESS_KEY\", None)\n",
    "\n",
    "\n",
    "# Get SageMaker jar files from the sagemaker_pyspark package.\n",
    "sagemaker_classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "# (Optional) If you want to force a particular AWS SDK version on the driver,\n",
    "# you can try adding a compatible AWS SDK bundle jar.\n",
    "# For example:\n",
    "# aws_sdk_bundle_jar = \"/path/to/aws-java-sdk-bundle-1.11.375.jar\"\n",
    "# combined_classpath = sagemaker_classpath + \":\" + aws_sdk_bundle_jar\n",
    "# Otherwise, we can continue using sagemaker_classpath alone.\n",
    "combined_classpath = sagemaker_classpath\n",
    "\n",
    "# Build the SparkSession.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"XGBoostSageMakerLocalExample\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", combined_classpath) \\\n",
    "    .config(\"spark.executor.extraClassPath\", combined_classpath) \\\n",
    "    .config(\"spark.jars\", combined_classpath) \\\n",
    "    .config(\"spark.driver.userClassPathFirst\", \"true\") \\\n",
    "    .config(\"spark.executor.userClassPathFirst\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"AKIAR3HUOTHCI6ZEO5FV\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"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\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Also set these explicitly in the Hadoop configuration.\n",
    "hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", \"AKIAR3HUOTHCI6ZEO5FV\")\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\",\n",
    "                \"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\")\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "hadoop_conf.set(\"fs.s3a.aws.credentials.provider\",\n",
    "                \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "# Define region and S3 path for training data.\n",
    "region = \"us-east-1\"\n",
    "data_path = \"s3a://sagemaker-sample-data-{}/spark/mnist/train/\".format(region)\n",
    "\n",
    "# Load training and test data in libsvm format.\n",
    "training_data = spark.read.format(\"libsvm\") \\\n",
    "    .option(\"numFeatures\", \"784\") \\\n",
    "    .load(data_path)\n",
    "test_data = spark.read.format(\"libsvm\") \\\n",
    "    .option(\"numFeatures\", \"784\") \\\n",
    "    .load(data_path)\n",
    "\n",
    "print(\"Training data schema:\")\n",
    "training_data.printSchema()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Configure the XGBoost SageMaker Estimator in local mode.\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Specify a training image known to work in local mode.\n",
    "# For example, use the Amazon ECR image for XGBoost:1.0-1-cpu-py3.\n",
    "# (Make sure you have Docker running so that local mode can pull and run this image.)\n",
    "training_image = \"382416733822.dkr.ecr.us-east-1.amazonaws.com/xgboost:1.0-1-cpu-py3\"\n",
    "\n",
    "xgboost_estimator = XGBoostSageMakerEstimator(\n",
    "    # Run training in local (Docker) mode.\n",
    "    trainingInstanceType=\"local\",\n",
    "    trainingInstanceCount=1,\n",
    "    endpointInstanceType=\"local\",           # Run inference in local mode.\n",
    "    endpointInitialInstanceCount=1,\n",
    "    sagemakerRole=IAMRole(\n",
    "        \"arn:aws:iam::127214197188:role/sagemaker-service-role\"),\n",
    "    trainingImage=training_image            # Force use of the specified image.\n",
    ")\n",
    "\n",
    "# Set hyperparameters for multi-class classification.\n",
    "xgboost_estimator.setObjective(\"multi:softmax\")\n",
    "xgboost_estimator.setNumRound(25)\n",
    "xgboost_estimator.setNumClasses(10)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train the model locally (using Docker) and get predictions.\n",
    "# --------------------------------------------------------------------\n",
    "xgboost_model = xgboost_estimator.fit(training_data)\n",
    "predictions = xgboost_model.transform(test_data)\n",
    "predictions.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data schema:\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4o\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize PySpark\n",
    "spark = SparkSession.builder.appName(\"SageMakerPySparkExample\").getOrCreate()\n",
    "\n",
    "# AWS default dataset (Change region if needed)\n",
    "region = \"us-east-1\"\n",
    "training_data_path = f\"s3a://sagemaker-sample-data-{region}/spark/mnist/train/\"\n",
    "\n",
    "# Load training data\n",
    "training_data = spark.read.format(\"libsvm\").option(\"numFeatures\", \"784\").load(training_data_path)\n",
    "\n",
    "test_data = spark.read.format(\"libsvm\").option(\"numFeatures\", \"784\").load(training_data_path)\n",
    "\n",
    "print(\"Training data schema:\")\n",
    "training_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data schema:\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# R1 \n",
    "from pyspark.sql import SparkSession\n",
    "import sagemaker_pyspark\n",
    "\n",
    "# Load SageMaker JARs\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "spark = SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath).getOrCreate()\n",
    "\n",
    "# Load sample data\n",
    "region = \"us-east-1\"\n",
    "training_data = spark.read.format(\"libsvm\") \\\n",
    "  .option(\"numFeatures\", \"784\") \\\n",
    "  .load(f\"s3a://sagemaker-sample-data-{region}/spark/mnist/train/\")\n",
    "\n",
    "test_data = spark.read.format(\"libsvm\") \\\n",
    "  .option(\"numFeatures\", \"784\") \\\n",
    "  .load(f\"s3a://sagemaker-sample-data-{region}/spark/mnist/train/\")\n",
    "print(\"Training data schema:\")\n",
    "training_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
